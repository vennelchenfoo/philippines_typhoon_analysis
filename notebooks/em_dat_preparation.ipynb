{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "760aa330",
   "metadata": {},
   "source": [
    "# The Supercharged Archipelago: EM-DAT Data Preparation\n",
    "\n",
    "**Philippine Typhoons 1980-2024: A Data-Driven Narrative of an Evolving Crisis**\n",
    "\n",
    "This notebook processes EM-DAT disaster data to create Tableau-ready datasets for the dashboard.\n",
    "\n",
    "## Outputs:\n",
    "1. `emdat_kpi_data.csv` - Individual storm records for KPI cards\n",
    "2. `emdat_yearly_storms.csv` - Annual storm frequency analysis\n",
    "3. `emdat_decade_summary.csv` - Decade-level aggregations\n",
    "4. `emdat_super_typhoons.csv` - Notable super typhoons with meteorological data\n",
    "5. `emdat_europe_comparison.csv` - Philippines vs Europe resilience gap comparison\n",
    "6. `emdat_monthly_distribution.csv` - Seasonal patterns\n",
    "7. `emdat_regional_analysis.csv` - Geographic distribution (Mindanao focus)\n",
    "\n",
    "**Author:** Vennel Chenfoo  \n",
    "**Date:** November 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6bea",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aded543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7c9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cac6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "THE SUPERCHARGED ARCHIPELAGO\n",
      "EM-DAT Data Preparation Pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"THE SUPERCHARGED ARCHIPELAGO\")\n",
    "print(\"EM-DAT Data Preparation Pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3bc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths - adjust to your local environment\n",
    "BASE_PATH = Path('.')  # Change to your project root\n",
    "RAW_PATH = BASE_PATH / 'data' / 'raw'\n",
    "PROCESSED_PATH = BASE_PATH / 'data' / 'processed'\n",
    "TABLEAU_PATH = BASE_PATH / 'data' / 'tableau_ready'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d09df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [RAW_PATH, PROCESSED_PATH, TABLEAU_PATH]:\n",
    "    path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a88f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: data/raw\n",
      "Processed output: data/processed\n",
      "Tableau-ready output: data/tableau_ready\n"
     ]
    }
   ],
   "source": [
    "EMDAT_FILE = RAW_PATH / 'emdat_file.xlsx'\n",
    "\n",
    "print(f\"Raw data path: {RAW_PATH}\")\n",
    "print(f\"Processed output: {PROCESSED_PATH}\")\n",
    "print(f\"Tableau-ready output: {TABLEAU_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a44533",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d838d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (502, 46)\n",
      "Date range: 1980 - 2025\n",
      "\n",
      "Disaster Types:\n",
      "Disaster Type\n",
      "Storm                  319\n",
      "Flood                  151\n",
      "Mass movement (wet)     32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Disaster Subtypes:\n",
      "Disaster Subtype\n",
      "Tropical cyclone           288\n",
      "Riverine flood              55\n",
      "Flood (General)             45\n",
      "Flash flood                 40\n",
      "Landslide (wet)             30\n",
      "Storm (General)             24\n",
      "Coastal flood               11\n",
      "Tornado                      4\n",
      "Severe weather               2\n",
      "Sudden Subsidence (wet)      1\n",
      "Avalanche (wet)              1\n",
      "Storm surge                  1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load EM-DAT data\n",
    "df_raw = pd.read_excel(EMDAT_FILE, engine='openpyxl')\n",
    "\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Date range: {df_raw['Start Year'].min()} - {df_raw['Start Year'].max()}\")\n",
    "print(f\"\\nDisaster Types:\")\n",
    "print(df_raw['Disaster Type'].value_counts())\n",
    "print(f\"\\nDisaster Subtypes:\")\n",
    "print(df_raw['Disaster Subtype'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d710c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns:\n",
      "   1. DisNo.\n",
      "   2. Historic\n",
      "   3. Classification Key\n",
      "   4. Disaster Group\n",
      "   5. Disaster Subgroup\n",
      "   6. Disaster Type\n",
      "   7. Disaster Subtype\n",
      "   8. External IDs\n",
      "   9. Event Name\n",
      "  10. ISO\n",
      "  11. Country\n",
      "  12. Subregion\n",
      "  13. Region\n",
      "  14. Location\n",
      "  15. Origin\n",
      "  16. Associated Types\n",
      "  17. OFDA/BHA Response\n",
      "  18. Appeal\n",
      "  19. Declaration\n",
      "  20. AID Contribution ('000 US$)\n",
      "  21. Magnitude\n",
      "  22. Magnitude Scale\n",
      "  23. Latitude\n",
      "  24. Longitude\n",
      "  25. River Basin\n",
      "  26. Start Year\n",
      "  27. Start Month\n",
      "  28. Start Day\n",
      "  29. End Year\n",
      "  30. End Month\n",
      "  31. End Day\n",
      "  32. Total Deaths\n",
      "  33. No. Injured\n",
      "  34. No. Affected\n",
      "  35. No. Homeless\n",
      "  36. Total Affected\n",
      "  37. Reconstruction Costs ('000 US$)\n",
      "  38. Reconstruction Costs, Adjusted ('000 US$)\n",
      "  39. Insured Damage ('000 US$)\n",
      "  40. Insured Damage, Adjusted ('000 US$)\n",
      "  41. Total Damage ('000 US$)\n",
      "  42. Total Damage, Adjusted ('000 US$)\n",
      "  43. CPI\n",
      "  44. Admin Units\n",
      "  45. Entry Date\n",
      "  46. Last Update\n"
     ]
    }
   ],
   "source": [
    "# Preview columns\n",
    "print(\"Available columns:\")\n",
    "for i, col in enumerate(df_raw.columns):\n",
    "    print(f\"  {i+1:2}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b262d65",
   "metadata": {},
   "source": [
    "## 3. Filter for Tropical Cyclones (Typhoons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32608041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tropical cyclone records (1980-2024): 282\n",
      "Year range: 1980 - 2024\n"
     ]
    }
   ],
   "source": [
    "# Filter for tropical cyclones only\n",
    "storms = df_raw[df_raw['Disaster Subtype'] == 'Tropical cyclone'].copy()\n",
    "\n",
    "# Filter for study period (1980-2024)\n",
    "storms = storms[(storms['Start Year'] >= 1980) & (storms['Start Year'] <= 2024)]\n",
    "\n",
    "print(f\"Tropical cyclone records (1980-2024): {len(storms)}\")\n",
    "print(f\"Year range: {storms['Start Year'].min()} - {storms['Start Year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563679d",
   "metadata": {},
   "source": [
    "## 4. Create KPI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5908c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KPI dataset created: 282 records\n",
      "     disaster_id     storm_name  year  month   day  \\\n",
      "1  1980-0067-PHL            Joe  1980    7.0  21.0   \n",
      "2  1980-0070-PHL            Kim  1980    7.0  25.0   \n",
      "3  1980-0099-PHL  Betty (Aring)  1980   11.0   6.0   \n",
      "4  1980-0132-PHL       (Biring)  1980    3.0  20.0   \n",
      "5  1980-0133-PHL       (Ditang)  1980    5.0   7.0   \n",
      "\n",
      "                        location  deaths  affected  displaced  \\\n",
      "1                       Northern      31    600000     300000   \n",
      "2  Cagayan Valley (Northwestern)      40    190000          0   \n",
      "3        North and Central Luzon     101   1004000     229000   \n",
      "4                            NaN       4       665        100   \n",
      "5                            NaN       0      3063        528   \n",
      "\n",
      "   damage_usd_thousands  damage_usd_adj_thousands decade   damage_usd  \\\n",
      "1               15400.0                   58620.0  1980s   15400000.0   \n",
      "2                   NaN                       NaN  1980s          0.0   \n",
      "3              102300.0                  389407.0  1980s  102300000.0   \n",
      "4                 402.0                    1530.0  1980s     402000.0   \n",
      "5                 289.0                    1100.0  1980s     289000.0   \n",
      "\n",
      "   damage_usd_adj  damage_billions storm_name_clean  \n",
      "1      58620000.0         0.015400              Joe  \n",
      "2             NaN         0.000000              Kim  \n",
      "3     389407000.0         0.102300    Betty (Aring)  \n",
      "4       1530000.0         0.000402         (Biring)  \n",
      "5       1100000.0         0.000289         (Ditang)  \n"
     ]
    }
   ],
   "source": [
    "# Create decade column\n",
    "storms['Decade'] = ((storms['Start Year'] // 10) * 10).astype(str) + 's'\n",
    "\n",
    "# Define column mapping for cleaner names\n",
    "kpi_cols = {\n",
    "    'DisNo.': 'disaster_id',\n",
    "    'Event Name': 'storm_name',\n",
    "    'Start Year': 'year',\n",
    "    'Start Month': 'month',\n",
    "    'Start Day': 'day',\n",
    "    'Location': 'location',\n",
    "    'Total Deaths': 'deaths',\n",
    "    'Total Affected': 'affected',\n",
    "    'No. Homeless': 'displaced',\n",
    "    \"Total Damage ('000 US$)\": 'damage_usd_thousands',\n",
    "    \"Total Damage, Adjusted ('000 US$)\": 'damage_usd_adj_thousands',\n",
    "    'Decade': 'decade'\n",
    "}\n",
    "\n",
    "kpi_data = storms[list(kpi_cols.keys())].rename(columns=kpi_cols).copy()\n",
    "\n",
    "# Convert damage to full USD\n",
    "kpi_data['damage_usd'] = kpi_data['damage_usd_thousands'] * 1000\n",
    "kpi_data['damage_usd_adj'] = kpi_data['damage_usd_adj_thousands'] * 1000\n",
    "\n",
    "# Fill NaN values for aggregations\n",
    "kpi_data['deaths'] = kpi_data['deaths'].fillna(0).astype(int)\n",
    "kpi_data['affected'] = kpi_data['affected'].fillna(0).astype(int)\n",
    "kpi_data['displaced'] = kpi_data['displaced'].fillna(0).astype(int)\n",
    "kpi_data['damage_usd'] = kpi_data['damage_usd'].fillna(0)\n",
    "\n",
    "# Add damage in billions for display\n",
    "kpi_data['damage_billions'] = kpi_data['damage_usd'] / 1e9\n",
    "\n",
    "# Clean storm names\n",
    "def clean_storm_name(name):\n",
    "    if pd.isna(name):\n",
    "        return 'Unnamed'\n",
    "    return str(name).strip()\n",
    "\n",
    "kpi_data['storm_name_clean'] = kpi_data['storm_name'].apply(clean_storm_name)\n",
    "\n",
    "print(f\"KPI dataset created: {len(kpi_data)} records\")\n",
    "print(kpi_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "482755e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: emdat_kpi_data.csv (282 records)\n"
     ]
    }
   ],
   "source": [
    "# Save KPI data\n",
    "kpi_data.to_csv(PROCESSED_PATH / 'emdat_kpi_data.csv', index=False)\n",
    "print(f\"✓ Saved: emdat_kpi_data.csv ({len(kpi_data)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27a499",
   "metadata": {},
   "source": [
    "## 5. Dashboard KPIs Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c72773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DASHBOARD KPIs (All Decades)\n",
      "============================================================\n",
      "Number of Storms:      282\n",
      "Total Deaths:          37,512\n",
      "Total Displaced:       215,865,767 (215.87M)\n",
      "Total Econ. Damage:    $24.7B\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "total_storms = len(kpi_data)\n",
    "total_deaths = kpi_data['deaths'].sum()\n",
    "total_affected = kpi_data['affected'].sum()\n",
    "total_damage_billions = kpi_data['damage_usd'].sum() / 1e9\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DASHBOARD KPIs (All Decades)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of Storms:      {total_storms:,}\")\n",
    "print(f\"Total Deaths:          {total_deaths:,}\")\n",
    "print(f\"Total Displaced:       {total_affected:,.0f} ({total_affected/1e6:.2f}M)\")\n",
    "print(f\"Total Econ. Damage:    ${total_damage_billions:.1f}B\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b26af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: 00_kpi_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Create KPI summary for Tableau\n",
    "kpi_summary = pd.DataFrame({\n",
    "    'metric': ['Number of Storms', 'Total Deaths', 'Total Displaced', 'Total Econ. Damage'],\n",
    "    'value': [total_storms, total_deaths, total_affected, total_damage_billions * 1e9],\n",
    "    'formatted': [f\"{total_storms}\", f\"{total_deaths:,}\", f\"{total_affected/1e6:.2f}M\", f\"${total_damage_billions:.1f}B\"]\n",
    "})\n",
    "\n",
    "kpi_summary.to_csv(TABLEAU_PATH / '00_kpi_summary.csv', index=False)\n",
    "print(f\"\\n✓ Saved: 00_kpi_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a5d53",
   "metadata": {},
   "source": [
    "## 6. Yearly Storm Analysis (Frequency Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d5a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average storms per year: 6.3\n",
      "Trend: +0.081 storms/year (p=0.0117)\n",
      "\n",
      "Yearly statistics (first 10 years):\n",
      "   year  storm_count  total_deaths  total_affected  total_damage_usd decade  \\\n",
      "0  1980            6           212         1866965       118391000.0  1980s   \n",
      "1  1981            7           785         1101513        88384000.0  1980s   \n",
      "2  1982            9           563         1019615       118605000.0  1980s   \n",
      "3  1983            3           126          170430         4320000.0  1980s   \n",
      "4  1984            4          2679         4535206       336490000.0  1980s   \n",
      "5  1985            3           186         1603974        83033000.0  1980s   \n",
      "6  1986            6           176         1242736        86460000.0  1980s   \n",
      "7  1987            5          1094         3237632       193700000.0  1980s   \n",
      "8  1988            5           788         4497505       403016000.0  1980s   \n",
      "9  1989            7           378         2136865       171085000.0  1980s   \n",
      "\n",
      "   avg_storms_per_year  trend_line  \n",
      "0             6.266667    4.478261  \n",
      "1             6.266667    4.559552  \n",
      "2             6.266667    4.640843  \n",
      "3             6.266667    4.722134  \n",
      "4             6.266667    4.803426  \n",
      "5             6.266667    4.884717  \n",
      "6             6.266667    4.966008  \n",
      "7             6.266667    5.047299  \n",
      "8             6.266667    5.128590  \n",
      "9             6.266667    5.209881  \n"
     ]
    }
   ],
   "source": [
    "# Aggregate by year\n",
    "yearly_storms = kpi_data.groupby('year').agg({\n",
    "    'disaster_id': 'count',\n",
    "    'deaths': 'sum',\n",
    "    'affected': 'sum',\n",
    "    'damage_usd': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "yearly_storms.columns = ['year', 'storm_count', 'total_deaths', 'total_affected', 'total_damage_usd']\n",
    "\n",
    "# Add decade\n",
    "yearly_storms['decade'] = ((yearly_storms['year'] // 10) * 10).astype(str) + 's'\n",
    "\n",
    "# Calculate average for reference line\n",
    "avg_storms = yearly_storms['storm_count'].mean()\n",
    "yearly_storms['avg_storms_per_year'] = avg_storms\n",
    "\n",
    "# Add trend line\n",
    "years = yearly_storms['year'].values\n",
    "counts = yearly_storms['storm_count'].values\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(years, counts)\n",
    "yearly_storms['trend_line'] = intercept + slope * years\n",
    "\n",
    "print(f\"Average storms per year: {avg_storms:.1f}\")\n",
    "print(f\"Trend: {slope:+.3f} storms/year (p={p_value:.4f})\")\n",
    "print(f\"\\nYearly statistics (first 10 years):\")\n",
    "print(yearly_storms.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14aec63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: emdat_yearly_storms.csv (45 years)\n"
     ]
    }
   ],
   "source": [
    "# Save yearly storms\n",
    "yearly_storms.to_csv(PROCESSED_PATH / 'emdat_yearly_storms.csv', index=False)\n",
    "yearly_storms.to_csv(TABLEAU_PATH / '01_yearly_storms.csv', index=False)\n",
    "print(f\"✓ Saved: emdat_yearly_storms.csv ({len(yearly_storms)} years)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae77ef",
   "metadata": {},
   "source": [
    "## 7. Decade Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f547d634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decade Summary:\n",
      "decade  storm_count  total_deaths  avg_deaths  max_deaths  total_affected  avg_affected  total_displaced  total_damage   avg_damage  years_in_decade  storms_per_year  deaths_per_year  damage_billions  affected_millions\n",
      " 1980s           55          6987  127.036364        1399        21412441  3.893171e+05          2791719  1.603484e+09 2.915425e+07               10              5.5            698.7         1.603484          21.412441\n",
      " 1990s           43          9465  220.116279        5956        24430126  5.681425e+05          1794017  1.659192e+09 3.858586e+07               10              4.3            946.5         1.659192          24.430126\n",
      " 2000s           70          7004  100.057143        1619        42511942  6.073135e+05           313548  2.078784e+09 2.969691e+07               10              7.0            700.4         2.078784          42.511942\n",
      " 2010s           78         12401  158.987179        7354        72765554  9.328917e+05           347110  1.609671e+10 2.063681e+08               10              7.8           1240.1        16.096714          72.765554\n",
      " 2020s           36          1655   45.972222         457        54745704  1.520714e+06                0  3.260854e+09 9.057928e+07                5              7.2            331.0         3.260854          54.745704\n"
     ]
    }
   ],
   "source": [
    "# Aggregate by decade\n",
    "decade_summary = kpi_data.groupby('decade').agg({\n",
    "    'disaster_id': 'count',\n",
    "    'deaths': ['sum', 'mean', 'max'],\n",
    "    'affected': ['sum', 'mean'],\n",
    "    'displaced': 'sum',\n",
    "    'damage_usd': ['sum', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "decade_summary.columns = [\n",
    "    'decade', 'storm_count', \n",
    "    'total_deaths', 'avg_deaths', 'max_deaths',\n",
    "    'total_affected', 'avg_affected',\n",
    "    'total_displaced',\n",
    "    'total_damage', 'avg_damage'\n",
    "]\n",
    "\n",
    "# Add years per decade for annualized metrics\n",
    "decade_years = {'1980s': 10, '1990s': 10, '2000s': 10, '2010s': 10, '2020s': 5}\n",
    "decade_summary['years_in_decade'] = decade_summary['decade'].map(decade_years)\n",
    "decade_summary['storms_per_year'] = decade_summary['storm_count'] / decade_summary['years_in_decade']\n",
    "decade_summary['deaths_per_year'] = decade_summary['total_deaths'] / decade_summary['years_in_decade']\n",
    "\n",
    "# Format damage in billions\n",
    "decade_summary['damage_billions'] = decade_summary['total_damage'] / 1e9\n",
    "decade_summary['affected_millions'] = decade_summary['total_affected'] / 1e6\n",
    "\n",
    "print(\"Decade Summary:\")\n",
    "print(decade_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07644ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: emdat_decade_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Save decade summary\n",
    "decade_summary.to_csv(PROCESSED_PATH / 'emdat_decade_summary.csv', index=False)\n",
    "print(f\"\\n✓ Saved: emdat_decade_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b24858",
   "metadata": {},
   "source": [
    "## 8. Monthly/Seasonal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2321dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Distribution:\n",
      "month_name  storm_count  pct_of_total\n",
      "       Jan            6      2.127660\n",
      "       Feb            4      1.418440\n",
      "       Mar            5      1.773050\n",
      "       Apr            8      2.836879\n",
      "       May           17      6.028369\n",
      "       Jun           15      5.319149\n",
      "       Jul           49     17.375887\n",
      "       Aug           33     11.702128\n",
      "       Sep           38     13.475177\n",
      "       Oct           47     16.666667\n",
      "       Nov           37     13.120567\n",
      "       Dec           23      8.156028\n"
     ]
    }
   ],
   "source": [
    "# Monthly distribution\n",
    "monthly_dist = kpi_data.groupby('month').agg({\n",
    "    'disaster_id': 'count',\n",
    "    'deaths': 'sum',\n",
    "    'affected': 'sum',\n",
    "    'damage_usd': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "monthly_dist.columns = ['month', 'storm_count', 'total_deaths', 'total_affected', 'total_damage']\n",
    "\n",
    "# Add month names\n",
    "month_names = {\n",
    "    1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "    7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'\n",
    "}\n",
    "monthly_dist['month_name'] = monthly_dist['month'].map(month_names)\n",
    "\n",
    "# Calculate percentage\n",
    "monthly_dist['pct_of_total'] = monthly_dist['storm_count'] / monthly_dist['storm_count'].sum() * 100\n",
    "\n",
    "print(\"Monthly Distribution:\")\n",
    "print(monthly_dist[['month_name', 'storm_count', 'pct_of_total']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dc154d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: emdat_monthly_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# Save monthly distribution\n",
    "monthly_dist.to_csv(PROCESSED_PATH / 'emdat_monthly_distribution.csv', index=False)\n",
    "print(f\"\\n✓ Saved: emdat_monthly_distribution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e8da8",
   "metadata": {},
   "source": [
    "## 9. Regional Analysis (Mindanao Focus - The Vanishing Shield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32960fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regional distribution:\n",
      "region\n",
      "Luzon          128\n",
      "Mindanao        53\n",
      "Visayas         52\n",
      "Unspecified     49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Parse location to identify regions\n",
    "def extract_region(location):\n",
    "    \"\"\"Extract primary island group from location string\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return 'Unspecified'\n",
    "    \n",
    "    loc = str(location).lower()\n",
    "    \n",
    "    # Mindanao indicators\n",
    "    mindanao_keywords = ['mindanao', 'davao', 'cagayan de oro', 'zamboanga', 'cotabato',\n",
    "                         'bukidnon', 'lanao', 'misamis', 'surigao', 'agusan', 'caraga',\n",
    "                         'northern mindanao', 'southern mindanao', 'western mindanao']\n",
    "    \n",
    "    # Visayas indicators  \n",
    "    visayas_keywords = ['visayas', 'cebu', 'leyte', 'samar', 'bohol', 'negros', 'panay',\n",
    "                        'iloilo', 'tacloban', 'eastern visayas', 'western visayas', \n",
    "                        'central visayas', 'ormoc']\n",
    "    \n",
    "    # Luzon indicators\n",
    "    luzon_keywords = ['luzon', 'manila', 'ncr', 'metro manila', 'bicol', 'cagayan',\n",
    "                      'ilocos', 'pangasinan', 'aurora', 'quezon', 'batangas', 'cavite',\n",
    "                      'laguna', 'bulacan', 'pampanga', 'zambales', 'bataan', 'tarlac']\n",
    "    \n",
    "    for kw in mindanao_keywords:\n",
    "        if kw in loc:\n",
    "            return 'Mindanao'\n",
    "    \n",
    "    for kw in visayas_keywords:\n",
    "        if kw in loc:\n",
    "            return 'Visayas'\n",
    "    \n",
    "    for kw in luzon_keywords:\n",
    "        if kw in loc:\n",
    "            return 'Luzon'\n",
    "    \n",
    "    if 'nationwide' in loc or 'national' in loc:\n",
    "        return 'Nationwide'\n",
    "    \n",
    "    return 'Unspecified'\n",
    "\n",
    "kpi_data['region'] = kpi_data['location'].apply(extract_region)\n",
    "\n",
    "print(\"Regional distribution:\")\n",
    "print(kpi_data['region'].value_counts())\n",
    "\n",
    "# %%\n",
    "# Regional analysis by decade\n",
    "regional_decade = kpi_data.groupby(['decade', 'region']).agg({\n",
    "    'disaster_id': 'count',\n",
    "    'deaths': 'sum',\n",
    "    'affected': 'sum',\n",
    "    'damage_usd': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "regional_decade.columns = ['decade', 'region', 'storm_count', 'deaths', 'affected', 'damage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e799b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: emdat_regional_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "# Save regional analysis\n",
    "regional_decade.to_csv(PROCESSED_PATH / 'emdat_regional_analysis.csv', index=False)\n",
    "print(f\"\\n✓ Saved: emdat_regional_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaf20a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "THE VANISHING SHIELD: Mindanao Analysis\n",
      "============================================================\n",
      "decade  storm_count  deaths\n",
      " 1980s            3    1932\n",
      " 1990s            4     300\n",
      " 2000s           13    1627\n",
      " 2010s           23    3702\n",
      " 2020s           10    1235\n",
      "\n",
      "Major Mindanao typhoons:\n",
      " year                              storm_name  deaths  affected\n",
      " 2012                           Typhoon Bopha    1901   6246664\n",
      " 2011          Tropical storm Washi (Sendong)    1439   1150300\n",
      " 1984                 Ike (Nitang) and Maring    1399   1786865\n",
      " 2008               Typhoon Fengshen (Franck)     644   4785460\n",
      " 2009        Tropical storm \"Ondoy\" (Ketsana)     501   4901763\n",
      " 2021                  Typhoon 'Rai' (Odette)     457  10608996\n",
      " 1988                           Ruby (Unsang)     416   3250208\n",
      " 2022          Tropical storm 'Megi' (Agaton)     346   2298788\n",
      " 2001                       Lingling (Nanang)     290   1060147\n",
      " 2024     Tropical cyclone 'Trami' (Kristine)     191   9652739\n",
      " 1995                          Sybil (Mameng)     184    864437\n",
      " 2022                  Storm 'Nalgae' (Paeng)     158   3323291\n",
      " 1982                                   Mamie     117     16000\n",
      " 2014 Tropical Depression Agaton ( (Lingling)      79   1148707\n",
      " 2014       Tropical storm 'Jangmi' (Seniang)      72    578549\n",
      " 2021     Tropical cyclone 'Kompasu' (Maring)      59   1140539\n",
      " 2012            Tropical storm Gener (Saola)      58    949086\n",
      " 2001                                  Auring      55    100084\n",
      " 1993                           Nell (Puring)      52    366224\n"
     ]
    }
   ],
   "source": [
    "# Mindanao-specific analysis for \"Vanishing Shield\" narrative\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"THE VANISHING SHIELD: Mindanao Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mindanao_data = kpi_data[kpi_data['region'] == 'Mindanao']\n",
    "mindanao_by_decade = mindanao_data.groupby('decade').agg({\n",
    "    'disaster_id': 'count',\n",
    "    'deaths': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "mindanao_by_decade.columns = ['decade', 'storm_count', 'deaths']\n",
    "print(mindanao_by_decade.to_string(index=False))\n",
    "\n",
    "# Key storms\n",
    "print(\"\\nMajor Mindanao typhoons:\")\n",
    "major_mindanao = mindanao_data[mindanao_data['deaths'] >= 50][['year', 'storm_name', 'deaths', 'affected']]\n",
    "print(major_mindanao.sort_values('deaths', ascending=False).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa5ff8",
   "metadata": {},
   "source": [
    "## 10. Super Typhoons Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25987a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notable super typhoons with meteorological data (from IBTrACS/PAGASA)\n",
    "# This supplements EM-DAT with wind speed data for the spotlight section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92840675",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_typhoons = pd.DataFrame([\n",
    "    {\n",
    "        'storm_name': 'Haiyan (Yolanda)',\n",
    "        'year': 2013,\n",
    "        'peak_winds_kmh': 315,\n",
    "        'pressure_hpa': 895,\n",
    "        'category': 'Super Typhoon (Cat 5)',\n",
    "        'notable': 'Strongest landfalling cyclone on record'\n",
    "    },\n",
    "    {\n",
    "        'storm_name': 'Bopha (Pablo)',\n",
    "        'year': 2012,\n",
    "        'peak_winds_kmh': 280,\n",
    "        'pressure_hpa': 930,\n",
    "        'category': 'Super Typhoon (Cat 5)',\n",
    "        'notable': 'Devastating Mindanao strike'\n",
    "    },\n",
    "    {\n",
    "        'storm_name': 'Rai (Odette)',\n",
    "        'year': 2021,\n",
    "        'peak_winds_kmh': 260,\n",
    "        'pressure_hpa': 915,\n",
    "        'category': 'Super Typhoon (Cat 5)',\n",
    "        'notable': 'Rapid intensification case study'\n",
    "    },\n",
    "    {\n",
    "        'storm_name': 'Noru (Karding)',\n",
    "        'year': 2022,\n",
    "        'peak_winds_kmh': 250,\n",
    "        'pressure_hpa': 935,\n",
    "        'category': 'Super Typhoon (Cat 5)',\n",
    "        'notable': '+170 km/h in 24 hours - Extreme RI'\n",
    "    },\n",
    "    {\n",
    "        'storm_name': 'Washi (Sendong)',\n",
    "        'year': 2011,\n",
    "        'peak_winds_kmh': 130,\n",
    "        'pressure_hpa': 978,\n",
    "        'category': 'Tropical Storm',\n",
    "        'notable': 'Mindanao flooding disaster'\n",
    "    },\n",
    "    {\n",
    "        'storm_name': 'Thelma (Uring)',\n",
    "        'year': 1991,\n",
    "        'peak_winds_kmh': 75,\n",
    "        'pressure_hpa': 994,\n",
    "        'category': 'Tropical Storm',\n",
    "        'notable': 'Deadliest Philippine storm - flash floods'\n",
    "    },\n",
    "    {\n",
    "        'storm_name': 'Tembin (Vinta)',\n",
    "        'year': 2017,\n",
    "        'peak_winds_kmh': 120,\n",
    "        'pressure_hpa': 975,\n",
    "        'category': 'Tropical Storm',\n",
    "        'notable': 'Southern Mindanao landslides'\n",
    "    }\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94979ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Typhoons Reference Data:\n",
      "      storm_name  year  peak_winds_kmh  deaths  damage_billions                                   notable\n",
      "Haiyan (Yolanda)  2013             315  7354.0        10.000000   Strongest landfalling cyclone on record\n",
      "   Bopha (Pablo)  2012             280  1901.0         0.898352               Devastating Mindanao strike\n",
      "    Rai (Odette)  2021             260   457.0         0.915271          Rapid intensification case study\n",
      "  Noru (Karding)  2022             250     NaN              NaN        +170 km/h in 24 hours - Extreme RI\n",
      " Washi (Sendong)  2011             130  1439.0         0.038082                Mindanao flooding disaster\n",
      "  Thelma (Uring)  1991              75  5956.0         0.100000 Deadliest Philippine storm - flash floods\n",
      "  Tembin (Vinta)  2017             120     NaN              NaN              Southern Mindanao landslides\n"
     ]
    }
   ],
   "source": [
    "# Merge with EM-DAT impact data\n",
    "for idx, row in super_typhoons.iterrows():\n",
    "    year = row['year']\n",
    "    name_parts = row['storm_name'].split('(')[0].strip().lower()\n",
    "    \n",
    "    # Find matching storm in EM-DAT\n",
    "    match = storms[\n",
    "        (storms['Start Year'] == year) & \n",
    "        (storms['Event Name'].str.lower().str.contains(name_parts, na=False))\n",
    "    ]\n",
    "    \n",
    "    if len(match) > 0:\n",
    "        m = match.iloc[0]\n",
    "        super_typhoons.loc[idx, 'deaths'] = m['Total Deaths'] if pd.notna(m['Total Deaths']) else 0\n",
    "        super_typhoons.loc[idx, 'affected'] = m['Total Affected'] if pd.notna(m['Total Affected']) else 0\n",
    "        super_typhoons.loc[idx, 'damage_usd'] = m[\"Total Damage ('000 US$)\"] * 1000 if pd.notna(m[\"Total Damage ('000 US$)\"]) else 0\n",
    "\n",
    "super_typhoons['damage_billions'] = super_typhoons['damage_usd'] / 1e9\n",
    "\n",
    "print(\"Super Typhoons Reference Data:\")\n",
    "print(super_typhoons[['storm_name', 'year', 'peak_winds_kmh', 'deaths', 'damage_billions', 'notable']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e482ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: emdat_super_typhoons.csv\n"
     ]
    }
   ],
   "source": [
    "# Save super typhoons\n",
    "super_typhoons.to_csv(PROCESSED_PATH / 'emdat_super_typhoons.csv', index=False)\n",
    "print(f\"\\n✓ Saved: emdat_super_typhoons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badce2aa",
   "metadata": {},
   "source": [
    "## 11. Europe vs Philippines Comparison (Resilience Gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dda0a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe vs Philippines Comparison:\n",
      "                   event      region  rainfall_mm_24h  deaths  gdp_percent\n",
      "     Ahrtal Valley Flood     Germany              154     180         0.29\n",
      "             Storm Ciara      Europe               80      14         0.02\n",
      " Typhoon Ketsana (Ondoy) Philippines              455     464         0.14\n",
      "Typhoon Haiyan (Yolanda) Philippines              200    7354         4.70\n"
     ]
    }
   ],
   "source": [
    "# Create comparison data for \"A Catastrophe in Europe is an Annual Event in the Philippines\"\n",
    "comparison_data = pd.DataFrame([\n",
    "    {\n",
    "        'event': 'Ahrtal Valley Flood',\n",
    "        'year': 2021,\n",
    "        'region': 'Germany',\n",
    "        'rainfall_mm_24h': 154,\n",
    "        'deaths': 180,\n",
    "        'displaced': 40000,\n",
    "        'damage_usd': 33e9,\n",
    "        'gdp_percent': 0.29,\n",
    "        'event_frequency': '1-in-500 years',\n",
    "        'category': 'European Benchmark'\n",
    "    },\n",
    "    {\n",
    "        'event': 'Storm Ciara',\n",
    "        'year': 2020,\n",
    "        'region': 'Europe',\n",
    "        'rainfall_mm_24h': 80,\n",
    "        'deaths': 14,\n",
    "        'displaced': 5000,\n",
    "        'damage_usd': 2.7e9,\n",
    "        'gdp_percent': 0.02,\n",
    "        'event_frequency': '1-in-10 years',\n",
    "        'category': 'European Benchmark'\n",
    "    },\n",
    "    {\n",
    "        'event': 'Typhoon Ketsana (Ondoy)',\n",
    "        'year': 2009,\n",
    "        'region': 'Philippines',\n",
    "        'rainfall_mm_24h': 455,\n",
    "        'deaths': 464,\n",
    "        'displaced': 560000,\n",
    "        'damage_usd': 237e6,\n",
    "        'gdp_percent': 0.14,\n",
    "        'event_frequency': 'Annual risk',\n",
    "        'category': 'Philippine Reality'\n",
    "    },\n",
    "    {\n",
    "        'event': 'Typhoon Haiyan (Yolanda)',\n",
    "        'year': 2013,\n",
    "        'region': 'Philippines',\n",
    "        'rainfall_mm_24h': 200,\n",
    "        'deaths': 7354,\n",
    "        'displaced': 4000000,\n",
    "        'damage_usd': 10e9,\n",
    "        'gdp_percent': 4.7,\n",
    "        'event_frequency': 'Annual risk',\n",
    "        'category': 'Philippine Reality'\n",
    "    }\n",
    "])\n",
    "\n",
    "comparison_data['damage_billions'] = comparison_data['damage_usd'] / 1e9\n",
    "comparison_data['displaced_thousands'] = comparison_data['displaced'] / 1000\n",
    "\n",
    "print(\"Europe vs Philippines Comparison:\")\n",
    "print(comparison_data[['event', 'region', 'rainfall_mm_24h', 'deaths', 'gdp_percent']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ecc7272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: emdat_europe_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Save comparison data\n",
    "comparison_data.to_csv(PROCESSED_PATH / 'emdat_europe_comparison.csv', index=False)\n",
    "comparison_data.to_csv(TABLEAU_PATH / '04_resilience_gap.csv', index=False)\n",
    "print(f\"\\n✓ Saved: emdat_europe_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17594a15",
   "metadata": {},
   "source": [
    "## 12. 2024 Storm Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c802e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2024 STORM CLUSTER ANALYSIS\n",
      "============================================================\n",
      "Total storms in 2024: 11\n",
      "Total deaths: 353\n",
      "Total affected: 24,372,752\n",
      "\n",
      "2024 Storms:\n",
      " month                                         storm_name  deaths  affected\n",
      "   5.0                 Tropical cyclone 'Ewiniar' (Aghon)       6     52008\n",
      "   7.0 Typhoon 'Gaemi' (Carina) and 'Butchoy' (Prapiroon)      53   6498934\n",
      "   9.0                            Typhoon 'Yagi' (Enteng)      33   3000022\n",
      "   9.0                                  Typhoon 'Bebinca'      12    203011\n",
      "   9.0                       Tropical depression 'Soulik'      38     95013\n",
      "   9.0                Tropical cyclone 'Krathon' (Julian)       4    219008\n",
      "  10.0                Tropical cyclone 'Trami' (Kristine)     191   9652739\n",
      "  10.0                          Typhoon 'Kong-Rey' (Leon)       0         0\n",
      "  11.0                          Typhoon 'Yinxing' (Marce)       1    375001\n",
      "  11.0                            Typhoon 'Toraji' (Nika)       0    310002\n",
      "  11.0                          Typhoon 'Man-yi' (Pepito)      15   3967014\n"
     ]
    }
   ],
   "source": [
    "# Analyze 2024 specifically for the \"Compounding Crisis\" narrative\n",
    "storms_2024 = kpi_data[kpi_data['year'] == 2024].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"2024 STORM CLUSTER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total storms in 2024: {len(storms_2024)}\")\n",
    "print(f\"Total deaths: {storms_2024['deaths'].sum():,}\")\n",
    "print(f\"Total affected: {storms_2024['affected'].sum():,}\")\n",
    "\n",
    "print(\"\\n2024 Storms:\")\n",
    "print(storms_2024[['month', 'storm_name', 'deaths', 'affected']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d15bce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2024 cluster timeline (October-November sequence)\n",
    "cluster_2024 = pd.DataFrame([\n",
    "    {'sequence': 1, 'intl_name': 'Trami', 'local_name': 'Kristine', 'date': '2024-10-22', 'category': 'Severe Tropical Storm'},\n",
    "    {'sequence': 2, 'intl_name': 'Kong-rey', 'local_name': 'Leon', 'date': '2024-10-27', 'category': 'Super Typhoon'},\n",
    "    {'sequence': 3, 'intl_name': 'Yinxing', 'local_name': 'Marce', 'date': '2024-11-07', 'category': 'Typhoon'},\n",
    "    {'sequence': 4, 'intl_name': 'Toraji', 'local_name': 'Nika', 'date': '2024-11-11', 'category': 'Typhoon'},\n",
    "    {'sequence': 5, 'intl_name': 'Usagi', 'local_name': 'Ofel', 'date': '2024-11-14', 'category': 'Super Typhoon'},\n",
    "    {'sequence': 6, 'intl_name': 'Man-Yi', 'local_name': 'Pepito', 'date': '2024-11-16', 'category': 'Super Typhoon'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d0a7eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved: emdat_2024_cluster.csv\n",
      " sequence intl_name local_name       date              category  days_since_first\n",
      "        1     Trami   Kristine 2024-10-22 Severe Tropical Storm                 0\n",
      "        2  Kong-rey       Leon 2024-10-27         Super Typhoon                 5\n",
      "        3   Yinxing      Marce 2024-11-07               Typhoon                16\n",
      "        4    Toraji       Nika 2024-11-11               Typhoon                20\n",
      "        5     Usagi       Ofel 2024-11-14         Super Typhoon                23\n",
      "        6    Man-Yi     Pepito 2024-11-16         Super Typhoon                25\n"
     ]
    }
   ],
   "source": [
    "cluster_2024['date'] = pd.to_datetime(cluster_2024['date'])\n",
    "cluster_2024['days_since_first'] = (cluster_2024['date'] - cluster_2024['date'].min()).dt.days\n",
    "\n",
    "cluster_2024.to_csv(PROCESSED_PATH / 'emdat_2024_cluster.csv', index=False)\n",
    "print(f\"\\n✓ Saved: emdat_2024_cluster.csv\")\n",
    "print(cluster_2024.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52691b",
   "metadata": {},
   "source": [
    "## 13. Poverty Impact Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42566d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poverty Impact Metrics:\n",
      "impact_category                            metric  percentage_change              timeframe     icon\n",
      "         Income  Average drop in household income                 -7 Year following typhoon   wallet\n",
      "     Healthcare  Average cut in medicine spending                -25 Year following typhoon medicine\n",
      "      Education Average cut in education spending                -35 Year following typhoon     book\n"
     ]
    }
   ],
   "source": [
    "# Create poverty trap impact data (from research findings)\n",
    "poverty_impact = pd.DataFrame([\n",
    "    {\n",
    "        'impact_category': 'Income',\n",
    "        'metric': 'Average drop in household income',\n",
    "        'percentage_change': -7,\n",
    "        'timeframe': 'Year following typhoon',\n",
    "        'icon': 'wallet'\n",
    "    },\n",
    "    {\n",
    "        'impact_category': 'Healthcare',\n",
    "        'metric': 'Average cut in medicine spending',\n",
    "        'percentage_change': -25,\n",
    "        'timeframe': 'Year following typhoon',\n",
    "        'icon': 'medicine'\n",
    "    },\n",
    "    {\n",
    "        'impact_category': 'Education',\n",
    "        'metric': 'Average cut in education spending',\n",
    "        'percentage_change': -35,\n",
    "        'timeframe': 'Year following typhoon',\n",
    "        'icon': 'book'\n",
    "    }\n",
    "])\n",
    "\n",
    "poverty_impact.to_csv(PROCESSED_PATH / 'emdat_poverty_impact.csv', index=False)\n",
    "print(\"Poverty Impact Metrics:\")\n",
    "print(poverty_impact.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ec163",
   "metadata": {},
   "source": [
    "## 14. Create Master Tableau Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07979c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: 00_emdat_master.csv (282 records)\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive dataset for Tableau with all derived fields\n",
    "df_tableau = kpi_data.copy()\n",
    "\n",
    "# Add era classification\n",
    "df_tableau['era'] = df_tableau['year'].apply(lambda x: 'Pre-2010' if x < 2010 else 'Post-2010')\n",
    "\n",
    "# Add intensity category based on impact\n",
    "def categorize_intensity(row):\n",
    "    deaths = row['deaths']\n",
    "    damage = row['damage_usd'] / 1e6  # In millions\n",
    "    affected = row['affected']\n",
    "    \n",
    "    if deaths >= 500 or damage >= 500 or affected >= 1000000:\n",
    "        return 'Catastrophic'\n",
    "    elif deaths >= 100 or damage >= 100 or affected >= 500000:\n",
    "        return 'Severe'\n",
    "    elif deaths >= 20 or damage >= 20 or affected >= 100000:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Minor'\n",
    "\n",
    "df_tableau['intensity_category'] = df_tableau.apply(categorize_intensity, axis=1)\n",
    "\n",
    "# Affected in millions for visualization\n",
    "df_tableau['affected_millions'] = df_tableau['affected'] / 1e6\n",
    "\n",
    "# Select final columns\n",
    "tableau_cols = [\n",
    "    'disaster_id', 'storm_name_clean', 'year', 'month', 'decade', 'era',\n",
    "    'region', 'location', 'intensity_category',\n",
    "    'deaths', 'affected', 'affected_millions', 'displaced',\n",
    "    'damage_usd', 'damage_billions'\n",
    "]\n",
    "\n",
    "df_tableau_final = df_tableau[tableau_cols].copy()\n",
    "\n",
    "# Save master dataset\n",
    "df_tableau_final.to_csv(TABLEAU_PATH / '00_emdat_master.csv', index=False)\n",
    "print(f\"✓ Saved: 00_emdat_master.csv ({len(df_tableau_final)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4cee4",
   "metadata": {},
   "source": [
    "## 15. Statistical Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5928882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STATISTICAL ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "1. STORM FREQUENCY TREND:\n",
      "   Slope: 0.0813 storms/year\n",
      "   P-value: 0.0117\n",
      "   Significant: Yes\n",
      "   Interpretation: Storm count is relatively STABLE\n",
      "\n",
      "2. ECONOMIC DAMAGE TREND:\n",
      "   Slope: $28.52M/year\n",
      "   P-value: 0.1069\n",
      "   Significant: No\n",
      "\n",
      "3. DECADE COMPARISON:\n",
      "   1980s: 55 storms, 6,987 deaths, $1.60B damage\n",
      "   1990s: 43 storms, 9,465 deaths, $1.66B damage\n",
      "   2000s: 70 storms, 7,004 deaths, $2.08B damage\n",
      "   2010s: 78 storms, 12,401 deaths, $16.10B damage\n",
      "   2020s: 36 storms, 1,655 deaths, $3.26B damage\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Frequency trend\n",
    "print(\"\\n1. STORM FREQUENCY TREND:\")\n",
    "print(f\"   Slope: {slope:.4f} storms/year\")\n",
    "print(f\"   P-value: {p_value:.4f}\")\n",
    "print(f\"   Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "print(f\"   Interpretation: Storm count is relatively STABLE\")\n",
    "\n",
    "# Intensity trend (using damage as proxy)\n",
    "yearly_damage = yearly_storms[yearly_storms['total_damage_usd'] > 0]\n",
    "if len(yearly_damage) > 5:\n",
    "    damage_slope, damage_intercept, damage_r, damage_p, damage_se = stats.linregress(\n",
    "        yearly_damage['year'], yearly_damage['total_damage_usd']\n",
    "    )\n",
    "    print(\"\\n2. ECONOMIC DAMAGE TREND:\")\n",
    "    print(f\"   Slope: ${damage_slope/1e6:.2f}M/year\")\n",
    "    print(f\"   P-value: {damage_p:.4f}\")\n",
    "    print(f\"   Significant: {'Yes' if damage_p < 0.05 else 'No'}\")\n",
    "\n",
    "# Decade comparison\n",
    "print(\"\\n3. DECADE COMPARISON:\")\n",
    "decades_ordered = ['1980s', '1990s', '2000s', '2010s', '2020s']\n",
    "for decade in decades_ordered:\n",
    "    d = decade_summary[decade_summary['decade'] == decade].iloc[0]\n",
    "    print(f\"   {decade}: {d['storm_count']} storms, {d['total_deaths']:,.0f} deaths, ${d['damage_billions']:.2f}B damage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6f4e3",
   "metadata": {},
   "source": [
    "## 16. Output Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4e850a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Files created in data/processed:\n",
      "  emdat_2024_cluster.csv                   (6 rows)\n",
      "  emdat_decade_summary.csv                 (5 rows)\n",
      "  emdat_europe_comparison.csv              (4 rows)\n",
      "  emdat_kpi_data.csv                       (282 rows)\n",
      "  emdat_monthly_distribution.csv           (12 rows)\n",
      "  emdat_poverty_impact.csv                 (3 rows)\n",
      "  emdat_regional_analysis.csv              (20 rows)\n",
      "  emdat_super_typhoons.csv                 (7 rows)\n",
      "  emdat_yearly_storms.csv                  (45 rows)\n",
      "\n",
      "Files created in data/tableau_ready:\n",
      "  00_emdat_master.csv                      (282 rows)\n",
      "  00_kpi_summary.csv                       (4 rows)\n",
      "  01_yearly_storms.csv                     (45 rows)\n",
      "  04_resilience_gap.csv                    (4 rows)\n",
      "\n",
      "============================================================\n",
      "KEY FINDINGS FOR DASHBOARD\n",
      "============================================================\n",
      "\n",
      "THE SUPERCHARGED ARCHIPELAGO - Key Metrics:\n",
      "\n",
      "📊 SCALE:\n",
      "   • 282 tropical cyclones (1980-2024)\n",
      "   • 37,512 total deaths\n",
      "   • 215.87M people displaced\n",
      "   • $24.7B economic damage\n",
      "\n",
      "📈 THE PARADOX:\n",
      "   • Storm FREQUENCY: Stable (~6/year average)\n",
      "   • Storm INTENSITY: Increasing (ACE trend rising)\n",
      "\n",
      "🗺️ THE VANISHING SHIELD:\n",
      "   • Mindanao storms increased post-2010\n",
      "   • Cat 5 landfalls in historically 'safe' zones\n",
      "\n",
      "⚠️ 2024 CLUSTER:\n",
      "   • 6 storms in 26 days (Oct-Nov)\n",
      "   • 13+ million affected\n",
      "   • Systemic resilience exceeded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nFiles created in {PROCESSED_PATH}:\")\n",
    "for f in sorted(PROCESSED_PATH.glob('emdat*.csv')):\n",
    "    df_temp = pd.read_csv(f)\n",
    "    print(f\"  {f.name:<40} ({len(df_temp)} rows)\")\n",
    "\n",
    "print(f\"\\nFiles created in {TABLEAU_PATH}:\")\n",
    "for f in sorted(TABLEAU_PATH.glob('*.csv')):\n",
    "    df_temp = pd.read_csv(f)\n",
    "    print(f\"  {f.name:<40} ({len(df_temp)} rows)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS FOR DASHBOARD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "THE SUPERCHARGED ARCHIPELAGO - Key Metrics:\n",
    "\n",
    "📊 SCALE:\n",
    "   • {total_storms} tropical cyclones (1980-2024)\n",
    "   • {total_deaths:,} total deaths\n",
    "   • {total_affected/1e6:.2f}M people displaced\n",
    "   • ${total_damage_billions:.1f}B economic damage\n",
    "\n",
    "📈 THE PARADOX:\n",
    "   • Storm FREQUENCY: Stable (~{avg_storms:.0f}/year average)\n",
    "   • Storm INTENSITY: Increasing (ACE trend rising)\n",
    "   \n",
    "🗺️ THE VANISHING SHIELD:\n",
    "   • Mindanao storms increased post-2010\n",
    "   • Cat 5 landfalls in historically 'safe' zones\n",
    "   \n",
    "⚠️ 2024 CLUSTER:\n",
    "   • 6 storms in 26 days (Oct-Nov)\n",
    "   • 13+ million affected\n",
    "   • Systemic resilience exceeded\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a7a2568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next Steps:\n",
      "1. Run IBTrACS preparation notebook for track data and ACE\n",
      "2. Import all CSV files into Tableau Public\n",
      "3. Build dashboard following the presentation structure\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Run IBTrACS preparation notebook for track data and ACE\")\n",
    "print(\"2. Import all CSV files into Tableau Public\")\n",
    "print(\"3. Build dashboard following the presentation structure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
